\chapter{Preprocessing}
\label{ch:prepr}

Quella che sarà descritta in questo capitolo è sicuramente la fase più impegnativa e delicata dell'intero lavoro. Vista quindi l'importanza che l'attività di \textit{preprocessing} ha rivestito, è stato scelto di descriverla con un elevato livello di dettaglio, evidenziando passaggio per passaggio le operazioni necessarie per dare all'insieme di dati grezzi una forma adeguata al tipo di analisi che ci si è prefissati di fare. \\

Nell'illustrare i vari procedimenti, per favorire una spiegazione lineare e il più possibile comprensibile, si impiegherà ancora la metafora della lavorazione meccanica, intesa in questo caso come una sgrossatura volta ad ottenere un semilavorato --- il data set \textit{preprocessato}, pronto per essere ulteriormente lavorato con le tecniche di \textit{data mining}. Ricordando quanto affermato nell'introduzione della sezioni precedenti, i dati iniziali rappresentano il pezzo grezzo da lavorare, mentre le tecnologie scelte gli utensili da impiegare nella sgrossatura.

\section{Preparazione dell'Ambiente di Lavoro}

	\subsection{Ottenere gli strumenti necessari}

		Innanzitutto è necessario predisporre gli utensili necessari al lavoro da svolgere --- fuor di metafora, si tratta di installare i programmi necessari al \textit{preprocessing}. Come abbiamo detto nella sezione dedicata alla \textit{technology stack}, abbiamo bisogno del \textit{d.b.m.s.} MongoDB e del suo \textit{driver}\footnote{il termine \textit{driver} non è perfettamente proprio per descrivere quella che in realtà è una semplice implementazione in Python delle \textit{A.P.I.} di MongoDB, ma colloquialmente rende bene l'idea della funzione di \texttt{pymongo}.}.

		La piattaforma impiegata è un personal computer con sistema operativo Arch Linux, perciò occorrerà installare MongoDB su di essa. Questo può essere fatto in modo estremamente agile, scaricando i pacchetti \texttt{mongodb} e \texttt{mongodb-tools} dalle repository ufficiali con il seguente comando:

		\begin{lstlisting}[language=bash,caption={installazione di MongoDB}]
			sudo pacman -S mongodb mongodb-tools --noconfirm
		\end{lstlisting}

		\vspace{0.3cm}

		Per ottenere \texttt{pymongo}, invece, occorre utilizzare il \textit{package manager} di Python, \texttt{pip}, invocandolo semplicemente come segue:

		\begin{lstlisting}[language=bash,caption={installazione di pymongo}]
			pip install pymongo
		\end{lstlisting}

		\vspace{0.3cm}

		A questo punto disponiamo degli utensili necessari per la nostra lavorazione.

	\subsection{Inizializzazione di un server MongoDB}

		Predisposti gli utensili, occorre adesso avviare la macchina e montare il pezzo --- in una \textit{vera} lavorazione meccanica, ovviamente \textit{prima} si monta l'utensile e si posiziona il pezzo, \textit{poi} si avvia la macchina; in questo caso, occorre avviare prima un processo di MongoDB affinché si possano importare i dati grezzi in uno \textit{schema} ed organizzarli in \textit{collections}, per poi lavorarli tramite \texttt{pymongo}. \\

		MongoDB fornisce un database estremamente veloce, e di default utilizza come supporto fisico di memorizzazione una cartella sul disco di installazione. La macchina utilizzata dispone di un disco a stato solido come unità di memoria non volatile, perciò le velocità di lettura e scrittura nel database di MongoDB risulterebbero ottime anche nella configurazione standard. Tuttavia, sia per migliorare ulteriormente le performances delle operazioni che per preservare la vita del disco\footnote{Le celle di memoria degli SSD, o dischi a stato solido, possono sopportare un numero limitato di scritture prima di rovinarsi.}, è stato scelto di creare un \textit{ramdisk}\footnote{Si tratta di un \textit{filesystem} implementato in un'area di RAM; è una tecnica per velocizzare estremamente le operazioni di lettura e scrittura, ma dato che il \textit{filesystem} è implementato su memoria volatile, i dati scritti in esso vengono persi dopo lo spegnimento della macchina.} da far utilizzare a MongoDB. \\

		Per pura comodità, le operazioni necessarie per realizzare quanto appena descritto sono state delegate ad uno script:

		\begin{lstlisting}[language=bash,caption={script di lancio di un server MongoDB}, numbers=left, stepnumber=1]
			#!/bin/zsh
			sudo killall mongod
			yes | rm -rf /mnt/ramdisk/db
			mkdir db /mnt/ramdisk/db
			mongod --dbpath=/mnt/ramdisk/db
		\end{lstlisting}

		\vspace{0.3cm}

		Nella \textit{shell} con cui è stato lanciato, si può vedere lo \textit{standard output} del processo \texttt{mongod}. Significa che MongoDB è attivo ed invocabile tramite gli strumenti a nostra disposizione.

	\subsection{Importazione dei Dati Grezzi}

		A questo punto, sia la macchina che gli utensili sono pronti: occorre posizionare il pezzo grezzo da lavorare, ovvero importare i dati in MongoDB. \\

		Come visto nel capitolo precedente, i dati a disposizione sono contenuti in otto file \texttt{csv}: si manterrà questa struttura --- almeno inizialmente --- importando quindi ogni file in una sua \textit{collection}. Come si può vedere di seguito, questa operazione è stata descritta nel \texttt{makefile} con l'etichetta \texttt{import}:

		\begin{center}
			\includegraphics[scale=0.7]{img/import.png}
		\end{center}

		Le variabili \texttt{\$(DB)} e \texttt{\$(PRDIR)} sono specifiche dell'ambiente predisposto\footnote{Per un esempio pratico, si consulti l'intero \texttt{makefile} riportato nell'Appendice \ref{appendix:makefile}.}. Essa si può lanciare con il seguente semplice comando di \texttt{shell}:

		\begin{lstlisting}[language=bash,caption={importazione dei dati in MongoDB}]
			make import
		\end{lstlisting}

		\vspace{0.3cm}

		La vera e propria invocazione del comando necessario all'import dei file nel database --- \texttt{mongoimport}, del pacchetto \texttt{mongodb-tools} --- è stata delegata in un file esterno al \texttt{makefile}, per preservare la compattezza e la brevità di quest'ultimo. Si veda comunque di seguito come è struturata la chiamata a \texttt{mongoimport} per uno degli otto file: 

		\begin{lstlisting}[language=bash,caption={dettaglio dell'importazione dei dati in MongoDB}]
			mongoimport -d exams -c rawStudentsPr1013 --type csv --file ../raw/prod_stud_10-11-12-13.csv --headerline
		\end{lstlisting}

		\vspace{0.3cm}

		Il comando, come ogni chiamata da \texttt{shell}, ha degli argomenti che ne specificano il comportamento. In questo caso, sono:

		\begin{itemize}
			\item \texttt{-d}: il database nel quale importare i dati;
			\item \texttt{-c}: la collection nella quale inserire i dati;
			\item \texttt{-type}: il tipo del file da leggere;
			\item \texttt{-file}: il riferimento al file;
			\item \texttt{--headerline}: indica che gli attributi delle istanze sono specificati nella prima riga del file.
		\end{itemize}

		A questo punto, nel database \texttt{exams} di MongoDB ci sono otto \textit{collections}, contenenti i \textit{documenti} che rappresentano le istanze dei dati a disposizione.

\section{Aggregazione per Anno Accademico}

	Avendo predisposto tutto, si può procedere con la prima lavorazione da fare. \\
	
	Si comincia intanto con l'ottenere dei data set \textit{minimali}, uno per le valutazioni dei corsi, l'altro per la produttività degli studenti, condensando in essi le informazioni principali contenute in quello delle prestazioni degli studenti in pochi parametri relativi ad un certo anno accademico. \\

	\subsection{Produttività degli Studenti}

		L'obiettivo è sintetizzare le informazioni sulla produttività degli studenti nei seguenti attributi:

		\begin{itemize}
			\item coorte di immatricolazione;
			\item numero di studenti totali;
			\item percentuale di studenti laureati;
			\item valutazione media ottenuta al test di ingresso;
			\item voto medio ottenuto agli esami;
			\item ritardo medio con cui è stato dato un esame.
		\end{itemize}

		Considerando il tipo dei dati a disposizione, occorrerà innanzitutto aggregare le istanze dei vari studenti in modo opportuno. Questo viene fatto utilizzando un modulo Python programmato \textit{ad hoc}, del quale si riporta il codice nell'Appendice \ref{appendix:aggregs}. \\
		
		L'oggetto \texttt{StudAggregator} definito in tale modulo viene impiegato per realizzare una prima aggregazione per corsi dei risultati dei singoli studenti. Si può usare questo risultato intermedio per ricavare le informazioni prefissate come necessarie, usando un altro apposito script Python riportato nell'Appendice \ref{appendix:stud}. Si è così prodotta una \textit{collection} che contiene esattamente il data set che ci si è prefissati di ottenere, calcolando un ritardo approssimativo medio (in semestri), la percentuale di studenti che hanno finito il corso di laurea nell'arco temporale a disposizione e le varie medie degli altri attributi.\\

		\begin{tabular}{llllll}
		\hline
		Coorte & N. & Laureati {[}\%{]} & Test Ingresso & Voto & Ritardo \\ \hline
		2010 & 30 & 6.67 & 15.4 & 25.5 & 0.81 \\
		2011 & 39 & 10.26 & 13.26 & 24.81 & 1.07 \\
		2012 & 58 & 25.86 & 14.05 & 24.79 & 1.01 \\
		2013 & 80 & 11.25 & 14.39 & 24.98 & 0.77 \\ \hline
		\end{tabular}

		\vspace{0.4cm}

		Il lancio di tutte queste operazioni è specificato nella ricetta \texttt{stud\_gen} del \texttt{makefile}.

	\subsection{Valutazione degli Insegnamenti}
	\label{prepr:eval_min}

		Analogamente a quanto fatto nella sezione immediatamente precedente, per questa famiglia di dati si vuole ottenere una aggregazione che riassuma i seguenti attributi:

		\begin{itemize}
			\item anno accademico
			\item numero di valutazioni registrate
			\item valutazione complessiva media dei corsi
			\item deviazione standard delle valutazioni
			\item percentuale delle valutazioni sufficienti
		\end{itemize}

		In questo caso è tutto più semplice, in quanto i dati relativi alla valutazione dei corsi sono già in forma aggregata: si tratta quindi solo di comprimerli ulteriormente, facendoli rientrare nello schema che ci si è prefissati. \\
		
		A tale proposito, si utilizzerà ancora il modulo \texttt{aggregs.py}, impiegandone stavolta un oggetto diverso: \texttt{ParAggregator}. Esso mantiene nella \textit{chiave primaria} i corsi, pertanto occorre aggregare ancora i dati, rendendo l'Anno Accademico l'unico parametro in grado di discriminare una tupla dall'altra. Tale operazione viene eseguita con un apposito script, riportato nell'Appendice \ref{appendix:eval}. \\

		Il risultato di quanto appena descritto, il cui lancio è stato specificato nella ricetta \texttt{teval\_gen} del \texttt{makefile}, produce un data set di questo tipo:\\

		\begin{tabular}{lllll}
		\hline
		A. A. & Val. Media & Std. Dev. Val. & Val. Sufficienti {[}\%{]} & N. \\ \hline
		2010-2011 & 7.54 & 1.74 & 82.14 & 17 \\
		2011-2012 & 7.93 & 1.61 & 90.68 & 26 \\
		2012-2013 & 7.98 & 1.7 & 90.55 & 30 \\
		... & ... & ... & ... & .. \\ \hline
		\end{tabular}

		\vspace{0.3cm}

\section{Join dei due insiemi di dati}

	Il \textit{join} delle due famiglie di dati è l'operazione più delicata fra quelle di tutto il \textit{preprocessing}. Occorre definire bene gli attributi sui quali definire la relazione, e prestare in generale attenzione ai vari errori che, se commessi, comprometterebbero totalmente il significato del risultato. \\

	\subsection{Join con valutazioni estese e attributi continui}

		Per prima cosa, si è realizzata una \textit{join} fra i due data set aggregando i dati degli studenti \textbf{per esame}, e i dati delle valutazioni dei corsi \textbf{per paragrafo}. Osservando l'albero delle dipendenze specificato nel \texttt{makefile} e mostrato in Figura \ref{make_join} --- che, si ripete, si può consultare per intero nell'Appendice \ref{appendix:makefile} --- è possibile notare che sono state prima realizzate le aggregazioni dei singoli data set e, solo successivamente, performata l'operazione di \textit{join} fra di essi. \\

		L'aggregazione dei dati degli studenti avviene in modo del tutto analogo a quanto fatto nella sezione precedente con l'oggetto \texttt{StudAggregator} del modulo Python \texttt{aggregs.py}. \\

		Per quanto riguarda l'aggregazione dei dati delle valutazioni dei corsi, sono state effettuati vari passaggi per arrivare a condensare le informazioni nei pochi attributi generali necessari. Tali passaggi sono visibili consultando nell'Appendice \ref{appendix:eval} il codice con cui sono stati realizzati. \\

		\begin{figure}
			\centering
			\caption{porzione del \texttt{makefile} che specifica le ricette per eseguire il \textit{join} fra le valutazioni dei corsi e i dati degli studenti}
			\label{make_join}
			\includegraphics[scale=0.80]{img/make_join.png}
		\end{figure} 

		Predisposti opportunamente i due insiemi di dati in due \textit{collection}, la fase di merge è stata portata avanti grazie all'ausilio di un altro modulo Python scritto \textit{ad hoc}: si tratta dell'oggetto \texttt{Merger}, contenuto nel file \texttt{merge.py}, la cui implementazione è visibile nell'Appendice \ref{appendix:merge}. Tale oggetto viene utilizzato nello script \texttt{dataset\_merge.py}. Il \textit{join} avviene, chiaramente, sugli attributi \textbf{Anno Accademico} e \textbf{Corso d'Esame}, trasferendo gli attributi specifici di una collezione direttamente nell'altra. \\

		A questo punto, la collezione risultante da queste operazioni ha le caratteristiche cercate. Tuttavia, presenta delle "sbavature" che è conveniente rimuovere prima di impiegarla efficacemente nel \textit{data mining}. Per eliminarle, ci si avvale di un ulteriore modulo Python realizzato per l'occasione, \texttt{cleanings.py}, riportato per intero nell'Appendice \ref{appendix:cleanings}. \\

		Concluso quanto appena descritto, si è creato un primo data set utilizzabile. Esso presenta i seguenti attributi, tutti di tipo	\textit{continuo} --- tranne ovviamente le \textit{chiavi primarie} e le istanze considerate per la produttività degli studenti:

		\begin{itemize}
			\item Anno Accademico
			\item Hash Docente/i
			\item Insegnamento
			\item \textbf{Produttività Studenti}:
				\subitem N \textit{[istanze]}
				\subitem Ritardo >=1sem \textit{[percent]}
				\subitem Ritardo \textit{[semestre, media]}
				\subitem Voto >= 24 \textit{[perc]}
				\subitem Voto \textit{[media]}
				\subitem Voto \textit{[std dev]}
			\item \textbf{Valutazione degli Aspetti Specifici del Corsi di Studi}:
				\subitem N \textit{[istanze]}
				\subitem Std Dev \textit{[media pesata]}
				\subitem Val >= 6 \textit{[percent]}
				\subitem Val \textit{[media pesata]}
			\item \textbf{Valutazione dell'adeguatezza delle Aule e delle Attrezzature}:
				\subitem \textit{attributi identici alla valutazione precedente}
			\item \textbf{Valutazione sul Docente}:
				\subitem \textit{attributi identici alla valutazione precedente}
			\item \textbf{Valutazione sulla Disponibilità di Informazioni Aggiuntive}:
				\subitem \textit{attributi identici alla valutazione precedente}
			\item \textbf{Valutazione dell'Organizzazione del Corso di Studi}:
				\subitem \textit{attributi identici alla valutazione precedente}
			\item \textbf{Valutazione dell'Organizzazione della Didattica}:
				\subitem \textit{attributi identici alla valutazione precedente}
			\item \textbf{Soddisfazione degli Studenti riguardo al Corso di Studi}:
				\subitem \textit{attributi identici alla valutazione precedente}
		\end{itemize}

		Come si può notare agilmente, il data set presenta un gran numero di attributi, molti fin troppo specifici per poterne estrarre un qualche tipo di informazione generale. Pertanto, è stato deciso aggregare ulteriormente fra loro gli attributi relativi alle valutazioni dei corsi, per sfoltire significativamente la quantità di campi presenti.
		
	\subsection{Join con valutazioni aggregate e attributi continui}

		Lavorando direttamente con uno script Python su sulla collezione appena creata, si è provveduto a ridurre il numero di attributi del data set aggregando fra loro le informazioni riguardanti i singoli paragrafi della valutazione dei corsi (si veda per un maggior dettaglio l'Appendice \ref{appendix:min}). \\

		A seguito di queste operazioni, si è ottenuta una \textit{collection} i cui documenti hanno sei attributi riguardanti la produttività degli studenti e quattro riguardo invece alle valutazioni dei corsi, sempre con \textit{chiave primaria} la tupla $< Anno \ Accademico, Insegnamento >$.
	
	\subsection{Attributi discreti}

		Come è stato spigato alla fine della fase di \textit{data understanding}, c'è bisogno di avere una versione \textit{discretizzata} dei data set dei quali è stata appena descritta la produzione. Questo è stato realizzato impiegando un altro modulo Python, \texttt{discretize.py}, la cui implementazione può essere consultata nell'Appendice \ref{appendix:discretize}. \\

		I \textit{range} nei quali far rientrare i valori continui degli attrbuti sono stati scelti cercando un giusto compromesso fra rappresentatività e sinteticità. Non è stato ovviamente possibile stabilirli a priori: quelli che si possono vedere nella sopra citata appendice sono il risutlato di un processo iterativo del tipo \textit{trial and error}, e sono risultati essere il miglior compromesso cercato.

\section{Sequenze ordinate di esami superati}

	Per utilizzare l'algoritmo \textit{GSP} messo a disposizione dalla suite di software Weka, c'è bisogno che i dati di input siano in un formato ben preciso. si trata del formato \texttt{arff}, il cui acrononomi sta a significare \textit{Attribute Relationship File Format}, cioè, traducendo quasi letteralmente, formato di file con attributi e relazioni. Le informazioni contenute in file di questo tipo sono memorizzate come \textit{plain text}, pertanto possono essere accedute da praticamente qualunque strumento.\\

	Per dare un'idea della qualità di questo requisito, un data set composto da due transazioni così composte: 
	
	\[tr. 1: \{ITEM1,\ ITEM2\} \\
	tr. 2: \{ITEM3\}\]

	\vspace{0.2cm}

	Dovrà essere rappresentato in questo modo: \\

	\noindent\texttt{1,ITEM1}\\
	\texttt{1,ITEM2}\\
	\texttt{2,ITEM3}\\

	Dato che non esistono strumenti di esportazione da MongoDB in grado di creare file di questo tipo, è stato programmato uno script \textit{ad hoc} che, ottenuti i dati degli studenti tramite delle interrogazioni sulla relativa collezione MongoDB, li trasforma in transazioni e li scrive direttametne su un file di testo avente la struttura richiesta. \\

	L'implementazione dello script sopra riferito è contenuta nell'Appendice \ref{appendix:seq}. Come si può notare dal codice, non ci si è limitati a produrre un singolo file contenente l'interezza delle informazioni a disposizione, ma si è provveduto a dividerlo in molte sezioni a seconda di vari criteri. Questo perché \textit{GSP} è un algoritmo che può essere estremamente pesante da eseguire su data set molto grandi: il problema della generazione degli \textit{itemset} frequenti è un problema $Np-Completo$\footnote{Significa che è un problema risolvibile in tempo polinomiale con algoritmi \textit{non deterministici}, cosa che implica la risolvibilità con algoritmi deterministici in tempo esponenziale.}, pertanto considerato intrattabile al crescere della dimensione dei dati di ingresso.\\

	Data la natura dell'analisi che si è prefissati di fare su questo tipo di dati, è risultato utile impiegare un primo, generale criterio di filtraggio: \textbf{si sono considerate solo le sequenze di esami di studenti che hanno completato almeno un terzo del Corso di Laurea}, cioè coloro che hanno conseguito almeno 60 Credifi Formativi Unitari. Questo perché le transazioni contenenti pochi \textit{item} vanno a contribuire all'analisi solo con sequenze corte, diluendo la frequenza di eventuali sequenze più lunghe che possono risultare interessanti. \\

	Stabilito questo principio, sono stati creati quindi i seguenti file, realizzati secondo ulteriori criteri di divisione: la \textbf{coorte di immatricolazione} e la \textbf{scuola superiore di provenienza}.

	\begin{itemize}
		\item Divisione per coorte di immatricolazione:
		\subitem \texttt{seq\_stud\_2010.arff}
		\subitem \texttt{seq\_stud\_2011.arff}
		\subitem \texttt{seq\_stud\_2012.arff}
		\subitem \texttt{seq\_stud\_2013.arff}
		\item Divisione successiva per scuola superiore di provenienza:
		\subitem \texttt{seq\_stud\_2010\_ist\_tecn.arff}
		\subitem \texttt{seq\_stud\_2010\_liceo.arff}
		\subitem \texttt{seq\_stud\_2011\_ist\_tecn.arff}
		\subitem \texttt{seq\_stud\_2011\_liceo.arff}
		\subitem \texttt{seq\_stud\_2012\_ist\_tecn.arff}
		\subitem \texttt{seq\_stud\_2012\_liceo.arff}
		\subitem \texttt{seq\_stud\_2013\_ist\_tecn.arff}
		\subitem \texttt{seq\_stud\_2013\_liceo.arff}
	\end{itemize}

	Inoltre, per puro scrupolo, è stato comunque creato un file contenente la totalità dei dati a disposizione --- fermo restando il criterio di filtraggio preliminare.

\section{Estrazione dei data set preprocessati}

	Ultimata finalmente la preparazione dei data set necessari alle successive analisi, occorre renderli disponibili in un formato che ne consenta una facile fruibilità. Al di là di necessità specifiche --- in particolare, il caso esposto nella sezione precedente ---  è stato scelto di esportare i dati da MongoDB in formato \texttt{csv}, direttamente importabile in Weka e accessibile in pratica da ogni tipo di programma dato che, come per i file \texttt{arff}, si tratta di informazioni codificate come \textit{plain text}. \\ 
	
	Questo fine è stato raggiunto abbbastanza agilmente esportando le varie collezioni tramite il comando \texttt{mongoexport}, comeè possibile vedere in Figura \ref{make_export}, mostrante una piccola porzione delle ricette del \texttt{makefile} relative a questo compito. \\

	\begin{figure}
			\centering
			\caption{porzione del \texttt{makefile} che specifica le ricette per eseguire il \textit{join} fra le valutazioni dei corsi e i dati degli studenti}
			\label{make_export}
		\includegraphics[scale=0.35]{img/export.png}
	\end{figure}

	Una parte significativa di questa operazione è rappresentata dalla richiesta di \texttt{mongoexport} di una lista degli attributi da esportare come parametro di input. Per evitare di doverla compilare manualmente, è stata utilizzata una \textit{query} sulla base di dati MongoDB per estrarre automaticamente la lista degli attributi dei documenti di tutte le collezioni da esportare.

	\lstdefinelanguage{JavaScript}{
  		keywords={break, case, catch, continue, debugger, default, delete, do, else, finally, for, function, if, in, instanceof, new, return, switch, this, throw, try, typeof, var, void, while, with},
 		morecomment=[l]{//},
  		morecomment=[s]{/*}{*/},
  		morestring=[b]',
  		morestring=[b]",
  		sensitive=true
	}

	\lstinputlisting[language=JavaScript,caption={script della shell di MongoDB per ottenere la lista degli attributi dei documenti in una collezione}, numbers=left, stepnumber=1]{../prepr/list_attr.mongosh}

	\vspace{0.3cm}

	Fatto questo, la fase di \textit{preprocessing} può dirsi conclusa. Si andrà quindi a lavorare sui data set prodotti, impiegando finalmente qualche tecnica di \textit{data mining}.
